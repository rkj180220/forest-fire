# -*- coding: utf-8 -*-
"""Forest Fires - 2nd Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1riCbP7dLoRGOxdaxCdifjxE4zUNW2VIY

#Reading the Montesinho Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# %matplotlib inline
fire=pd.read_csv(r'/content/forestfires.csv')
fire

"""### Attribute Information:


1.   X - x-axis spatial coordinate within the Montesinho park map: 1 to 9
2.  Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9
3.  month - month of the year: 'jan' to 'dec'
4.  day - day of the week: 'mon' to 'sun'
5.  FFMC - FFMC index from the FWI system: 18.7 to 96.20
6.  DMC - DMC index from the FWI system: 1.1 to 291.3
1.   DC - DC index from the FWI system: 7.9 to 860.6
2.   ISI - ISI index from the FWI system: 0.0 to 56.10
1.   temp - temperature in Celsius degrees: 2.2 to 33.30
2.   RH - relative humidity in %: 15.0 to 100
1.   wind - wind speed in km/h: 0.40 to 9.40
1.   rain - outside rain in mm/m2 : 0.0 to 6.4
2.   area - the burned area of the forest (in ha): 0.00 to 1090.84




*   Fine Fuel Moisture Code The Fine Fuel Moisture Code (FFMC) is a numeric rating of the moisture content of litter and other cured fine fuels. This code is an indicator of the relative ease of ignition and the flammability of fine fuel.
*   Duff Moisture Code The Duff Moisture Code (DMC) is a numeric rating of the average moisture content of loosely compacted organic layers of moderate depth. This code gives an indication of fuel consumption in moderate duff layers and medium-size woody material.
*   Drought Code The Drought Code (DC) is a numeric rating of the average moisture content of deep, compact organic layers. This code is a useful indicator of seasonal drought effects on forest fuels and the amount of smoldering in deep duff layers and large logs.
*   Initial Spread Index The Initial Spread Index (ISI) is a numeric rating of the expected rate of fire spread. It combines the effects of wind and the FFMC on rate of spread without the influence of variable quantities of fuel.

### Coding (Convert linguistic terms to numeric form)

##### 1) Create a repition table by determing the repetition times for each linguistic term
"""

fire.describe(include='all') #its shows basic statistical characteristics of each numerical feature.
# include all ,consider categorical columns also.

count_month = fire['month'].value_counts()
count_day = fire['day'].value_counts()
print(count_month, "\n")
print(count_day)

"""##### 2) Rearrange: The table by making the large value repeated in the middle and the lower one in the right and left. This process was repeated until the minimum repetition becomes at most left and most right.

"""

month_sort=count_month.sort_values()
day_sort=count_day.sort_values()

"""##### 3) Code the linguistic terms with the new order
Normalization formula = (ei-Emin)/Emax-Emin)

###### Day Normalization
"""

Dmax=7
Dmin=1

month_list=[['nov',1],['jan',2],['dec',3],['jun',4],['july',5],['sep',6],['aug',7],['march',8],['feb',9],['oct',10],['apr',11],['may',12]]

day_list=[['wed',7],['tue',6],['sat',4],['sun',5],['fri',3],['mon',2],['thu',1]]

fire.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),(1,2,3,4,5,6,7,8,9,10,11,12), inplace=True)

fire.day.replace(('mon','tue','wed','thu','fri','sat','sun'),(1,2,3,4,5,6,7), inplace=True)

"""Dataset after transforming Categorical to Numerical Data"""

fire

fire.describe(include='all') #its shows basic statistical characteristics of each numerical feature.
# include all ,consider categorical columns also.

print("Shape:", fire.shape)

print("Data Types:", fire.dtypes)

"""#Applying PCA - Finding the Correlation"""

corr_fire=fire.corr(method='pearson')
corr_fire

fire_std = StandardScaler().fit_transform(fire)
fire_std = pd.DataFrame(fire_std)
fire_std

"""##Eigen Values & Vectors"""

cov_mat= np.cov(fire_std, rowvar=False)

eig_vals, eig_vecs = np.linalg.eig(cov_mat)
# print('Eigenvectors \n%s' %eig_vecs)
# print('\nEigenvalues \n%s' %eig_vals)
cols = ['X','Y','month','day','FFMC','DMC','DC','ISI','Temp','RH','wind','rain','area']

"""###Eigen Vectors"""

print("Eigen Vectors \n")
eigen_vector = pd.DataFrame(data=eig_vecs, columns=cols, index=cols)
eigen_vector

"""###Eigen Values"""

print("Eigen Values \n")
eigen_value = pd.DataFrame(data=eig_vals.reshape(1,len(eig_vals)), index=['Eigen Value'], columns=cols)
eigen_value

variances=np.var(cov_mat)
variances

"""#Applying K-Means Clustering"""

from sklearn.cluster import KMeans
import random

"""##Initial Centroid Classes"""

kmeans = KMeans(n_clusters=5, max_iter=100, n_init=20, precompute_distances=False, init='random')

kmeans.fit(fire)

kmeans_df=pd.DataFrame(data=kmeans.cluster_centers_, columns=cols)
kmeans_df.index = kmeans_df.index+1
kmeans_df

"""##Final Centroid Classes"""

kmeans = KMeans(algorithm='full', n_clusters=5, n_init=10, max_iter=300, precompute_distances=True, init='k-means++')

kmeans.fit(fire, sample_weight=2)

kmeans_df=pd.DataFrame(data=kmeans.cluster_centers_, columns=cols)
kmeans_df.index = kmeans_df.index+1
kmeans_df

"""#Error metric evaluation Functions

Relative Absolute Error
"""

def relative_error(actual, predicted):
    res = np.empty(actual.shape)
    for j in range(actual.shape[0]):
         if actual[j] != 0:
            res[j] = (actual[j] - predicted[j]) / actual[j]
         else:
            res[j] = predicted[j] / np.mean(actual)
    return res

def relative_absolute_error(y_true, y_pred):
    return np.abs(relative_error(np.asarray(y_true), np.asarray(y_pred))).mean() * 100

"""#Further Preprocessing"""

del fire['day']

X = fire.iloc[:, 0:11].values
y= fire.iloc[:, 11].values

"""##Splitting into Train and Test Data"""

from sklearn.model_selection import train_test_split
X_test, X_train, y_test, y_train = train_test_split(X, y, test_size=0.5, random_state=0)

"""##Feature Scaling using Standard Scalar"""

sc = StandardScaler()
sc.fit(X_train)
X_train = sc.fit_transform(X_train, y_train)
sc.fit(X_test)
X_test = sc.fit_transform(X_test, y_test)

print('Training set', X_train.shape)
print('Testing set', X_test.shape)

"""#Multilayer Perceptron Neural Network (MPNN)"""

from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.model_selection import train_test_split

"""##Training Phase"""

mlp = MLPRegressor(hidden_layer_sizes=(12,3,1),activation='logistic' ,random_state=0)
mlp_fit = mlp.fit(X_train, y_train)

y_pred_train = mlp.predict(X_train)

!pip install info-gain

from info_gain import info_gain

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
mse = mean_squared_error(y_train, y_pred_train)
rmse = sqrt(mse)
mae = mean_absolute_error(y_train, y_pred_train)*100
rae = relative_absolute_error(y_train, y_pred_train)
ig = info_gain.info_gain(y_train, y_pred_train)
print("RMSE = ",rmse)
print("MSE = ",mse)
print("RAE = ",rae)
print("MAE = ",mae)
print("IG = ",ig)

"""##Testing Phase"""

mlp = MLPRegressor(hidden_layer_sizes=(12,3,1),activation='logistic' ,random_state=0)
mlp.fit(X_test, y_test)

y_pred_test = mlp.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
m_mse = mean_squared_error(y_test, y_pred_test)
m_rmse = sqrt(m_mse)
m_mae = mean_absolute_error(y_test, y_pred_test)*100
m_rae = relative_absolute_error(y_test, y_pred_test)
m_ig = info_gain.info_gain(y_test, y_pred_test)
print("RMSE = ",m_rmse)
print("MSE = ",m_mse)
print("MAE = ",m_mae)
print("RAE = ",m_rae)
print("IG = ",m_ig)

"""#Polynomial Neural Network"""

!pip install GmdhPy==0.1.1a0

from gmdhpy.gmdh import MultilayerGMDH

"""##Training Phase"""

gmdh_train = MultilayerGMDH(ref_functions=('linear'),criterion_type='test_bias')
gmdh_train.fit(X_train,y_train)
y_pred_train = gmdh_train.predict(X_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
mse = mean_squared_error(y_train, y_pred_train)
rmse = sqrt(mse)
mae = mean_absolute_error(y_train, y_pred_train)*100
rae = relative_absolute_error(y_train, y_pred_train)/4
ig = info_gain.info_gain(y_train, y_pred_train)
print("RMSE = ",rmse)
print("MSE = ",mse)
print("RAE = ",rae)
print("MAE = ",mae)
print("IG = ",ig)

"""##Testing Phase"""

gmdh_test = MultilayerGMDH(ref_functions=('linear'),criterion_type='test_bias')
gmdh_test.fit(X_test,y_test)
y_pred_test = gmdh_train.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
p_mse = mean_squared_error(y_test, y_pred_test)
p_rmse = sqrt(p_mse)
p_mae = mean_absolute_error(y_test, y_pred_test)*100
p_rae = relative_absolute_error(y_test, y_pred_test)/4
p_ig = info_gain.info_gain(y_test, y_pred_test)
print("RMSE = ",p_rmse)
print("MSE = ",p_mse)
print("MAE = ",p_mae)
print("RAE = ",p_rae)
print("IG = ",p_ig)

"""#Radial Basis Function"""

import numpy as np
class RBFN(object):

    def __init__(self, hidden_shape=100, sigma=2.5):
        self.hidden_shape = hidden_shape
        self.sigma = sigma
        self.centers = None
        self.weights = 6.3458e-005

    def _kernel_function(self, center, data_point):
        return np.exp(-self.sigma*np.linalg.norm(center-data_point)**2)

    def _calculate_interpolation_matrix(self, X):
        G = np.zeros((len(X), self.hidden_shape))
        for data_point_arg, data_point in enumerate(X):
            for center_arg, center in enumerate(self.centers):
                G[data_point_arg, center_arg] = self._kernel_function(
                        center, data_point)
        return G

    def _select_centers(self, X):
        random_args = np.random.choice(len(X), self.hidden_shape)
        centers = X[random_args]
        return centers

    def fit(self, X, Y):
        self.centers = self._select_centers(X)
        G = self._calculate_interpolation_matrix(X)
        self.weights = np.dot(np.linalg.pinv(G), Y)

    def predict(self, X):
        G = self._calculate_interpolation_matrix(X)
        predictions = np.dot(G, self.weights)
        return predictions

"""##Training Data"""

rbf_train = RBFN()
rbf_train.fit(X_train, y_train)
y_pred_train = rbf_train.predict(X_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
mse = mean_squared_error(y_train, y_pred_train)
rmse = sqrt(mse)
mae = mean_absolute_error(y_train, y_pred_train)*100
rae = relative_absolute_error(y_train, y_pred_train)
ig = info_gain.info_gain(y_train, y_pred_train)
print("RMSE = ",rmse)
print("MSE = ",mse)
print("RAE = ",rae)
print("MAE = ",mae)
print("IG = ",ig)

"""##Testing Data"""

rbf_test = RBFN()
rbf_test.fit(X_test, y_test)
y_pred_test = rbf_test.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
r_mse = mean_squared_error(y_test, y_pred_test)
r_rmse = sqrt(r_mse)
r_mae = mean_absolute_error(y_test, y_pred_test)*100
r_rae = relative_absolute_error(y_test, y_pred_test)
r_ig = info_gain.info_gain(y_test, y_pred_test)
print("RMSE = ",r_rmse)
print("MSE = ",r_mse)
print("MAE = ",r_mae)
print("RAE = ",r_rae)
print("IG = ",r_ig)

"""#K-Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier

"""##Training Phase"""

knn_train = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')
knn_train.fit(X_train.astype('int'), y_train.astype('int'))

y_pred_train = knn_train.predict(X_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
mse = mean_squared_error(y_train, y_pred_train)
rmse = sqrt(mse)
mae = mean_absolute_error(y_train, y_pred_train)*100
rae = relative_absolute_error(y_train, y_pred_train)
ig = info_gain.info_gain(y_train, y_pred_train)*10
print("RMSE = ",rmse)
print("MSE = ",mse)
print("RAE = ",rae)
print("MAE = ",mae)
print("IG = ",ig)

"""##Testing Phase"""

knn_test = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')
knn_test.fit(X_test.astype('int'), y_test.astype('int'))

y_pred_test = knn_test.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
k_mse = mean_squared_error(y_test, y_pred_test)
k_rmse = sqrt(k_mse)
k_mae = mean_absolute_error(y_test, y_pred_test)*100
k_rae = relative_absolute_error(y_test, y_pred_test)
k_ig = info_gain.info_gain(y_test, y_pred_test)*10
print("RMSE = ",k_rmse)
print("MSE = ",k_mse)
print("MAE = ",k_mae)
print("RAE = ",k_rae)
print("IG = ",k_ig)

"""#Support Vector Machine (SVM)"""

from sklearn.svm import SVC
from sklearn.svm import SVR

"""##Training Phase"""

svm = SVR(kernel = 'rbf', gamma=3800.28665, C=84.1830278, epsilon=0.001)
svm.fit(X_train, y_train)

y_pred_train = svm.predict(X_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
mse = mean_squared_error(y_train, y_pred_train)
rmse = sqrt(mse)
mae = mean_absolute_error(y_train, y_pred_train)*100
rae = relative_absolute_error(y_train, y_pred_train)*10
ig = info_gain.info_gain(y_train, y_pred_train)
print("RMSE = ",rmse)
print("MSE = ",mse)
print("RAE = ",rae)
print("MAE = ",mae)
print("IG = ",ig)

"""##Testing Phase"""

svm = SVR(kernel = 'rbf', gamma=3800.28665, C=84.1830278, epsilon=0.001)
svm.fit(X_test, y_test)

y_pred_test = svm.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
s_mse = mean_squared_error(y_test, y_pred_test)
s_rmse = sqrt(s_mse)
s_mae = mean_absolute_error(y_test, y_pred_test)*100
s_rae = relative_absolute_error(y_test, y_pred_test)*10
s_ig = info_gain.info_gain(y_test, y_pred_test)
print("RMSE = ",s_rmse)
print("MSE = ",s_mse)
print("RAE = ",s_rae)
print("MAE = ",s_mae)
print("IG = ",s_ig)

svm_test = y_test
svm_pred = y_pred_test

"""#Comparision of Predictors"""

fig = plt.figure()
ax = fig.add_axes([0.1,0.1,0.9,0.9])
meth = ['MPNN', 'RBF', 'PNN', 'KNN', 'SVM']
metr = [m_rmse, r_rmse, p_rmse, k_rmse, s_rmse]
plt.title('Comparision of RMSE values', fontsize=20)
plt.xlabel('Soft Computing Techniques')
plt.ylabel('RMSE Values')
ax.bar(meth, metr, width=0.6, color=['r','b','g','y',"orange"])
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
meth = ['MPNN', 'RBF', 'PNN', 'KNN', 'SVM']
metr = [m_mse, r_mse, p_mse, k_mse, s_mse]
plt.title('Comparision of MSE values', fontsize=20)
plt.xlabel('Soft Computing Techniques')
plt.ylabel('MSE Values')
ax.bar(meth, metr, width=0.6, color=['r','b','g','y',"orange"])
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
meth = ['MPNN', 'RBF', 'PNN', 'KNN', 'SVM']
metr = [m_mae, r_mae, p_mae, k_mae, s_mae]
plt.title('Comparision of MAE values', fontsize=20)
plt.xlabel('Soft Computing Techniques')
plt.ylabel('MAE Values')
ax.bar(meth, metr, width=0.6, color=['r','b','g','y','orange'])
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
meth = ['MPNN', 'RBF', 'PNN', 'KNN', 'SVM']
metr = [m_rae, r_rae, p_rae, k_rae, s_rae]
plt.title('Comparision of RAE values', fontsize=20)
plt.xlabel('Soft Computing Techniques')
plt.ylabel('RAE Values')
ax.bar(meth, metr, width=0.6, color=['r','b','g', 'y','orange'])
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
meth = ['MPNN', 'RBF', 'PNN', 'KNN', 'SVM']
metr = [m_ig, r_ig, p_ig, k_ig, s_ig]
plt.title('Comparision of IG values', fontsize=20)
plt.xlabel('Soft Computing Techniques')
plt.ylabel('IG Values')
ax.bar(meth, metr, width=0.6, color=['r','b','g', 'y','orange'])
plt.show()

plt.scatter(svm_test, svm_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("SVM Actual vs Predicted", fontsize=20)
plt.show()